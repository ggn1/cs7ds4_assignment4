1. The conceptual base of information theory.
1.1. Shannon's break-through.
1.2. Coding.
1.3. Entropy.
1.4. Divergence and redundancy.
1.5. Semantics and information theory.
1.6. Mutual information.
1.7. On definitions of entropy, divergence and mutual information.
1.8. Data reduction, side information.
1.9. Mixing.
1.10. Compression and correlated data.

2. Beyond Yes and No, modelling continuous data.
2.1. Rate distortion theory.
2.2. The qubit.
2.3. Entanglement.

3. Duality between truth and description.
3.1. Elements of game theroy.
3.2. Games of information.
3.3. The maximum entropy principle.
3.4. Universal coding.
3.5. Other games iof information.
3.6. Maximum entropy in physics.
3.7. Gibbs conditioning principle.
3.8. Applications in statistics.
3.9. Law of large numbers and central limit theorem.

4. Is capacity only useful for engineers??
4.1. Capacity.
4.2. Quantum capacities.

5. Implementations with lack of theory: Multi-user communication.
5.1. The multiple access channel.
5.2. Broadcast problems.

6. Conclusions